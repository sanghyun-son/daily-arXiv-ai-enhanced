name: arXiv-crawl-and-submit

on:
  schedule:
    # 18:00 KST = 9:00 UTC (crawl and submit batch)
    - cron: "0 9 * * *"
  workflow_dispatch:

jobs:
  crawl-and-submit:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Install dependencies
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        uv sync
        
    - name: Crawl arXiv papers
      id: crawl_step
      run: |
        source .venv/bin/activate
        today=$(date -u "+%Y-%m-%d")
        echo "Starting to crawl $today arXiv papers..."
        
        # Check if today's file exists, delete if found
        if [ -f "data/${today}.jsonl" ]; then
            echo "ðŸ—‘ï¸ Found existing today's file, deleting for fresh start..."
            rm "data/${today}.jsonl"
            echo "âœ… Deleted existing file: data/${today}.jsonl"
        else
            echo "ðŸ“ Today's file doesn't exist, ready to create new one..."
        fi
        
        cd daily_arxiv
        export OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}
        export OPENAI_BASE_URL=${{ secrets.OPENAI_BASE_URL }}
        export LANGUAGE="${{ vars.LANGUAGE }}"
        export CATEGORIES="${{ vars.CATEGORIES }}"
        export MODEL_NAME="${{ vars.MODEL_NAME }}"
        export INTERESTS="${{ vars.INTERESTS }}"
        
        # Use Scrapy to crawl
        scrapy crawl arxiv -o ../data/${today}.jsonl
        
        # Check if crawling was successful
        if [ ! -f "../data/${today}.jsonl" ]; then
            echo "Crawling failed, no data file generated"
            exit 1
        fi
        
        echo "Crawling completed"
        
        # Perform intelligent deduplication check
        echo "Performing intelligent deduplication check..."
        python daily_arxiv/daily_arxiv/check_stats.py --date ${today}
        dedup_exit_code=$?
        
        case $dedup_exit_code in
            0)
                echo "Deduplication check passed, continuing with batch submission"
                ;;
            1)
                echo "No new content found, stopping workflow"
                exit 1
                ;;
            2)
                echo "Error in deduplication, stopping workflow"
                exit 2
                ;;
            *)
                echo "Unknown exit code, stopping workflow"
                exit 1
                ;;
        esac
        
        echo "crawl_date=$today" >> $GITHUB_OUTPUT
        
    - name: Submit batch job for AI processing
      run: |
        source .venv/bin/activate
        today=${{ steps.crawl_step.outputs.crawl_date }}
        echo "Submitting batch job for AI processing..."
        
        cd ai
        export OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}
        export OPENAI_BASE_URL=${{ secrets.OPENAI_BASE_URL }}
        export LANGUAGE="${{ vars.LANGUAGE }}"
        export MODEL_NAME="${{ vars.MODEL_NAME }}"
        export INTERESTS="${{ vars.INTERESTS }}"
        
        # Submit actual batch job to OpenAI
        python submit_batch.py --data ../data/${today}.jsonl
        
        # Check if batch submission was successful
        if [ $? -ne 0 ]; then
            echo "Batch job submission failed"
            exit 1
        fi
        
        # Create a marker file to indicate batch job is submitted
        echo "$(date -u)" > "../data/${today}_batch_submitted.txt"
        echo "Batch job submitted for date: $today"
        
    - name: Commit crawl results
      run: |
        git config --global user.email "${{ vars.EMAIL }}"
        git config --global user.name "${{ vars.NAME }}"
        git add data/
        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes to commit"
          exit 0
        fi
        git commit -m "crawl: $(date -u '+%Y-%m-%d') arXiv papers - batch job submitted"
        git push origin main
        echo "Crawl results committed and pushed"
